---
title: "p8105_hw3_xs2330"
author: "Xiao Shi"
date: "October 14, 2018"
output: github_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(p8105.datasets)
```

## Probelm 1
**Data cleaning**
```{r p1 data cleaning}
brfss = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  mutate(response = as.factor(response))
```

**States with 7 locations**
```{r p1 questions}
brfss_2002 = filter(brfss, year == 2002) %>%
  select(locationabbr, locationdesc) %>%
  distinct() %>%
  count(locationabbr)
states_7locs = brfss_2002[which(brfss_2002$n == 7), 1]
states_7locs
```
The above three states have 7 observations

**Spaghetti plot of locations**
```{r p1 spaghetti plot}
brfss_obser = select(brfss, year, locationabbr, locationdesc) %>%
  group_by(year) %>%
  count(locationabbr)
ggplot(brfss_obser, aes(x = year, y = n, color = locationabbr)) +
  geom_line() +
  labs(
    title = "Number of Observations in each state from 2002 to 2010",
    x = "Year",
    y = "Number of Observations",
    caption = "Data from the p8105 package"
  ) +
  viridis::scale_colour_viridis(
    name = "locationabbr", 
    discrete = TRUE
  ) +
  theme_set(theme_bw() + theme(legend.position = "right"))
```

**2002, 2006, 2010 mean table for "Excellent" responses**
```{r p1 proportion table}
prop_table_mean = filter(brfss, year == 2002 | year == 2006 | year == 2010) %>%
  filter(locationabbr == "NY") %>%
  filter(response == "Excellent") %>%
  select(year, locationdesc, response, data_value) %>%
  group_by(year) %>%
  summarize(mean_excellent = mean(data_value)) %>% 
  spread(key = year, value = mean_excellent) %>% 
  knitr::kable(digits = 1)
```

**2002, 2006, 2010 sd table for "Excellent" responses**
```{r}
prop_table_sd = filter(brfss, year == 2002 | year == 2006 | year == 2010) %>%
  filter(locationabbr == "NY") %>%
  filter(response == "Excellent") %>%
  select(year, locationdesc, response, data_value) %>%
  group_by(year) %>%
  summarize(sd_excellent = sd(data_value)) %>% 
  spread(key = year, value = sd_excellent) %>% 
  knitr::kable(digits = 1)
```

**Five-panel plot for each response category over time**
```{r five panel, warning = FALSE}
five_panel = select(brfss, year, locationabbr, locationdesc, response, data_value) %>%
  group_by(year, locationabbr, response) %>%
  summarize(respon_mean = mean(data_value))
ggplot(five_panel, aes(x = year, y = respon_mean, color = response)) +
  geom_point() +
  labs(
    title = "Number of Observations in each state from 2002 to 2010",
    x = "Year",
    y = "Number of Observations",
    caption = "Data from the p8105 package"
  ) +
  facet_grid(~response) +
  viridis::scale_colour_viridis(
    name = "response", 
    discrete = TRUE
  ) +
  theme_set(theme_bw() + theme(legend.position = "bottom")) +
  theme(axis.text.x = element_text(angle = 90))
```
The above 5-panel plot shows the average proportion in each response category, and the distribution of these state-level averages over time. 

## Problem 2
```{r instacart data}
inscart = instacart
summary(inscart)
```
The `instacart` dataset consist of `r nrow(inscart)` rows, and `r ncol(inscart)` columns. Each row in the dataset is a product from an order. The dataset provided detialed information from the customers' side such as user ID by `user_id`, and the hour of day on which the order was placed by `order_dow`. On the other side, the dataset also provided detialed information from the seller's side such as aisle ID by `aisle_id`, and the aisle name by `aisle`.

How many aisles are there, and which aisles are the most items ordered from?
Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

**Number of aisles**
```{r}
num_aisle = nrow(distinct(inscart, aisle_id))
```
The number of aisle is `r num_aisle`

**items ordered in each aisle**
```{r}
num_item_aisle = inscart %>%
  count(aisle) %>%
  mutate(aisle = fct_reorder(aisle, desc(n)))
ggplot(num_item_aisle, aes(x = aisle, y = n)) +
    geom_point() +
    theme(axis.text.x = element_text(angle = 90))
```

**Popular item aisles**
```{r}
popular_aisles = filter(inscart, aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  group_by(aisle, product_name) %>%
  count(product_name)
popular_aisles_baking = filter(popular_aisles, aisle == "baking ingredients")
most_popular_baking = popular_aisles_baking[which.max(popular_aisles_baking$n),2]
popular_aisles_dog = filter(popular_aisles, aisle == "dog food care")
most_popular_dog = popular_aisles_dog[which.max(popular_aisles_dog$n),2]
popular_aisles_veggie = filter(popular_aisles, aisle == "packaged vegetables fruits")
most_popular_veggie = popular_aisles_veggie[which.max(popular_aisles_veggie$n),2]
popular_aisles = cbind.data.frame(most_popular_veggie, most_popular_dog, most_popular_baking)
colnames(popular_aisles) = c("packaged vegetables fruits", "dog food care", "baking ingredients")
knitr::kable(popular_aisles)
```

Above table shows the most popular item aisles in “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

**Pink Lady Apples and Coffee Ice Cream orders by day**
```{r}
mean_hour_apple_coffee = select(inscart, order_id, order_dow, order_hour_of_day, product_name) %>%
  mutate(order_dow = factor(order_dow, labels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))) %>%
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>%
  group_by(order_dow, product_name) %>%
  janitor::clean_names() %>%
  summarise(mean_order_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_order_hour)
knitr::kable(mean_hour_apple_coffee)  
```

## Problem 3
```{r data original summary}
skimr::skim(ny_noaa)
```
The ny noaa data were accessed from the NOAA National Climatic Data Center, consisting of `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)`. Key variables include date by `date`, prcipitation by `prcp`, snowfall by `snow`, depth of snow by `snwd`, highest temparture by `tmax`, and lowest temparture by `tmin`. For `tmax` and `tmin`, `r mean(is.na(ny_noaa$tmax))` of the data is missing, which is a big problem if we are trying to calculate temparature related results because we cannot be sure whether the missing data would lead to a different result.

**data cleaning**
```{r p3 data cleaning}
ny_noaa_cleaned = separate(ny_noaa, date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(tmax = as.integer(tmax)) %>%
  mutate(tmax = tmax/10) %>%
  mutate(tmin = as.integer(tmin)) %>%
  mutate(tmin = tmin/10) %>%
  mutate(prcp = prcp/10) 
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
} 
snowmode = getmode(ny_noaa$snow)
```
The most commonly observed value for snowfall is `r snowmode`, because it's  not always winter in NYC

**NYC average maximum temperature in Jan and July**
```{r, warning = FALSE}
tmax_jan_july = select(ny_noaa_cleaned, id, year, month, tmax, tmin) %>%
  filter(month == "01" | month == "07") %>%
  mutate(month = factor(month, labels = c("Jan", "July"))) %>%
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax, na.rm = TRUE),
            mean_tmin = mean(tmin, na.rm = TRUE))
ggplot(tmax_jan_july, aes(x = year, y = mean_tmax, color = month)) +
  geom_boxplot() +
  facet_grid(~month) +
  theme(axis.text.x = element_text(angle = 90)) +
  labs(
    title = "NYC average maximum temperature in Jan and July",
    x = "Year",
    y = "Average maximum temperature",
    caption = "Data from the noaa package"
  )
```

From the two-panel plot showing the average temperature in January and in July in each station across years. The average maximum temperature in both January and July fluctuates around a certain level through out 3 decades. Outliers in January are seen on the higher temperature side, and outliers in July are seen on the lower temperature side.

**Max temperature vs. Min temperature in NYC**

two panel plot to show tmax vs tmin for the full dataset
```{r, warning = FALSE}
ggplot(ny_noaa_cleaned, aes(x = tmax, y = tmin)) +
  geom_bin2d() +
  labs(
    title = "Max temperature vs. Min temperature in NYC",
    x = "Max temperature",
    y = "Min temperature",
    caption = "Data from the noaa package"
  )
```

A heat map was chosen because we are trying to display a lot of matched pairs of data (max temp vs. min temp).

**Snowfall greater than 0 and less than 100 separately by year**
```{r}
snow_fall = select(ny_noaa_cleaned, id, month, year, snow) %>%
  filter(snow > 0 & snow < 100)
ggplot(snow_fall, aes(x = year, y = snow, fill = "red")) +
  geom_violin() +
  labs(
    title = "Snowfall from 0 to 100 mm in NYC",
    x = "Year",
    y = "Average snowfall",
    caption = "Data from the noaa package"
  ) +
  theme(axis.text.x = element_text(angle = 90))

```

A violin graph was chosen because the snowfall is a variable without much change throughout the years. Thus the violin graph is able to show the fact that snowfalls are stable through out the years, but also able to show the tiny difference between years. For example, the year 2010 has a different shape around 37 mm.
